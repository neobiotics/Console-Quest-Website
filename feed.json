{
    "version": "https://jsonfeed.org/version/1",
    "title": "Console Quest",
    "description": "",
    "home_page_url": "https://consolequest.net",
    "feed_url": "https://consolequest.net/feed.json",
    "user_comment": "",
    "author": {
        "name": "Juri Steiner"
    },
    "items": [
        {
            "id": "https://consolequest.net/three-nas-builds-three-months.html",
            "url": "https://consolequest.net/three-nas-builds-three-months.html",
            "title": "The Great Hardware Shuffle: Three NAS Builds in Three Months",
            "summary": "Let me pause for a second… this is a lot to unpack.",
            "content_html": "<p>Let me pause for a second… this is a lot to unpack.</p><p>Not long ago, I was living a happy, content life. My janky-ass NAS chugging along nicely. My beautifully cable-managed, premier bang-for-the-buck gaming PC running smoothly and quietly. Me being zen in computer heaven.</p><p>Then, a series of events started to happen.</p><h2 id=\"it-all-started-with-an-m1-mac-mini\">It All Started With an M1 Mac Mini</h2>\n<p>I inherited an M1 Mac Mini. Now, I’ve been a Mac user for a very long time - in fact, the professional part of my IT shenanigans started as a repair technician at an Apple Premium Reseller. I used iPhones and iPads ever since, and only stopped using Macs when my maxed-out MacBook Pro from 2013 stopped getting security updates. I wasn’t willing to pay what felt like ridiculous amounts of money just for “having a computer,” especially since I always had a Windows PC for gaming and now had a ThinkPad from work.</p><p>So naturally, I got accustomed to our new household addition very quickly. Only then I started to notice that it seems I mostly only used my gaming PC for daily driving, not really for gaming. The small inconvenience that my monitor has three video inputs but only two USB connections to its KVM functionality started to add up to the nagging minutes I had to wait for Windows and game updates every time I did eventually fire up the gaming PC.</p><p>The next event in this series was the long-awaited fiber internet installation. Cloud gaming, like NVIDIA’s GeForce Now - which I’d always rejected because of poor latency and bad quality - suddenly became feasible. Sessions of playing Cyberpunk 2077 on raytracing “psycho” setting, something my Radeon RX 6750 XT was far from capable of, further took away from the little time I spent with my actual gaming PC.</p><h2 id=\"the-brilliant-plan-that-wasnt\">The Brilliant Plan (That Wasn’t)</h2>\n<p>Then came what I thought was the big revelation.</p><p>I’d been experimenting with local LLM inference for a while, but without much success, being restricted to throwaway stuff from work like a Quadro P2000. You get something running, but you don’t get very far on a nearly decade-old card with 5GB of VRAM.</p><p>It came to me almost like an epiphany: “Just stick the Radeon into the NAS and chug away with 12GB of VRAM! ROCm for the win!”</p><p>I installed Ubuntu on the gaming PC, spun up an Ollama container with the <code>HSA_OVERRIDE_GFX_VERSION=10.3.0</code> parameter, and it worked like a charm. Then I realized my NAS only has a single PCIe slot, which was already occupied by the HBA.</p><p>I hesitated for about three seconds.</p><p>Then I convinced myself that using the gaming PC as-is would be brilliant because it has a nice case with plenty of drive bays, tons of case fans, a good power supply, and a way faster CPU and RAM wouldn’t hurt either. And to be fair, the thing I called my NAS until that point was so hacky, even I was quietly convinced that something would fail catastrophically and, most importantly, without good reason.</p><p>I quickly glossed over the fact that doing so would add more than 20W to my idle power consumption and started swapping drives.</p><h2 id=\"when-physics-disagreed\">When Physics Disagreed</h2>\n<p>The drive swap went smoothly. I schlepped the heavy case downstairs to connect everything up. This is where the drama started to unfold.</p><p>I knew it wouldn’t fit into the network cabinet because the case was too deep, but I figured I could just put it on top vertically. It didn’t fit. It <em>just barely</em> didn’t fit. It also didn’t fit on the cabinet next to the network cabinet because it interfered with the doors. It didn’t fit on the floor.</p><p>At one point I even placed it on the washing machine behind the laundry basket, which my wife immediately dismissed. A good thing in hindsight, because mechanical hard drives on a washing machine in spin cycle is clear Type 3 fun.</p><p>When I finally found a place behind the door, installed the door bumper so you don’t slam the door into the PC every time you enter the room, and finished routing network and power cabling behind several household appliances, I finally sat down to test some LLMs.</p><p>I loaded up a model, typed in a prompt, the tension rose…</p><p>Only to be ruptured by loud beeping noises from downstairs.</p><p>Who could have known? The UPS overload protection tripped, and my terminal fell silent. Only the penetrating beeping prevailed.</p><p>I can’t even tell you how I felt in that moment, because my brain is so desperately trying to repress the memory of that day.</p><h2 id=\"the-temporary-solution\">The Temporary Solution</h2>\n<p>A couple of days later, I got my composure back, powered the NAS from the home battery storage (which doubles as a UPS), and actually used this setup for a couple of weeks. While obviously totally overpowered, it all worked as intended.</p><p>Only I noticed that immediately after the hardware swap, I kept getting container health alerts from Netdata multiple times every day, directly followed by “all good now” messages. Although the issue didn’t seem to produce any outages, I nevertheless tried to diagnose it.</p><p>Using the log output Dockge provides didn’t help much because I rarely managed to catch the issue in time. This threw me down a completely different rabbit hole of installing Graylog, which I’ll probably cover in another blog article.</p><p>I tinkered with it for a while, unsuccessfully trying to bring down the idle power consumption. Eventually I decided the power consumption wasn’t worth it and went back to my old platform.</p><h2 id=\"the-return-and-then-some\">The Return… And Then Some</h2>\n<p>I migrated the Home Assistant Assistant (I’m not making that one up, it’s actually called that way) and Paperless-GPT to OVH AI Endpoints. I put the old motherboard back into the “new” case, and the container health warnings went away.</p><p>I thought about going back to my old method of “retaining the motherboard” but hesitated. The 3D printed “test bench” I was using was almost wrecked (I’d never seen such bad layer cohesion in a bought 3D print - the whole piece was a complete ripoff), and the HDDs had been screwed together with parts of scrap plastic, while the SSDs were just dangling around on their SATA cables.</p><p>The next thing that happened was the ridiculous DDR5 price hike, which finally helped me overcome the emotional hurdle of selling the remaining parts of my gaming PC. Something that was strangely difficult for me, having always possessed a gaming PC for more than three decades - the better part of my life now.</p><h2 id=\"the-research-that-led-nowhere\">The Research That Led Nowhere</h2>\n<p>The prospect of soon earning some not insignificant amount of money immediately made me justify being completely fed up with the “PC behind the door” situation. I jumped into researching case options for a proper rack-mounted NAS the minute I decided to sell the PC components.</p><p>So I set up a research prompt and let Claude do its thing.</p><p>Claude came back with essentially empty hands. Everything it researched was either not inside the specs I gave it or hallucinated. After giving it a couple days of occasional research, I concluded that there are no sub-30cm rack mount cases that can hold the amount of hardware I wished for. I considered buying an additional JBOD, but even that seemed difficult under the space constraints I had.</p><h2 id=\"the-5€-decision-that-haunted-me\">The 5€ Decision That Haunted Me</h2>\n<p>Then I remembered this marvel of technology. A thing I’d found years ago before buying that horrible 3D printed thing because it was 5€ cheaper.</p><p>I don’t even want to use the word “hindsight” anymore.</p><p>I initially planned to buy these nice PCIe slot HDD holders, which would have been a really compact but rather pricey solution, costing around 10€ per piece. I was pondering over different HDD mounting options like drive cages and brackets and the likes, when I noticed that the rack shelves I was using have these very convenient “mounting holes.”</p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://consolequest.net/media/posts/9/mounting-holes.jpeg\" alt=\"Rack shelf ventilation holes with nylon washers used for hard drive mounting\" width=\"4032\" height=\"3024\" sizes=\"(min-width: 760px) 660px, calc(93.18vw - 30px)\" srcset=\"https://consolequest.net/media/posts/9/responsive/mounting-holes-xs.jpeg 320w ,https://consolequest.net/media/posts/9/responsive/mounting-holes-sm.jpeg 480w ,https://consolequest.net/media/posts/9/responsive/mounting-holes-md.jpeg 768w ,https://consolequest.net/media/posts/9/responsive/mounting-holes-xl.jpeg 1024w\"></figure><p>One Amazon Prime delivery later, I was happily mounting hard drives beneath the rack shelf. In the end, I had to add some nylon washers because the screws I ordered were too long, because why measure anything when you can just eyeball everything, right? At least I can tell myself the washers are for vibration isolation.</p><h2 id=\"the-final-form\">The Final Form</h2>\n<p>The test bench motherboard holder perfectly fit into the rack shelf after removing the screws that were sticking out the side, and everything came together perfectly. I installed four 120mm case fans in the designated holes of the network cabinet, hooked them up to the mainboard with some extension cables, tidied up the whole rack, and stood back, admiring what I’d accomplished.</p><figure class=\"post__image\"><img loading=\"lazy\" src=\"https://consolequest.net/media/posts/9/rack.jpeg\" alt=\"Network rack showing UniFi Switch, Raspberry Pi, NAS in test bench, and somewhat organized cables\" width=\"3024\" height=\"4032\" sizes=\"(min-width: 760px) 660px, calc(93.18vw - 30px)\" srcset=\"https://consolequest.net/media/posts/9/responsive/rack-xs.jpeg 320w ,https://consolequest.net/media/posts/9/responsive/rack-sm.jpeg 480w ,https://consolequest.net/media/posts/9/responsive/rack-md.jpeg 768w ,https://consolequest.net/media/posts/9/responsive/rack-xl.jpeg 1024w\"></figure><p>The third hardware swap within mere months was done, and I was really happy with how it turned out in the end.</p><h2 id=\"current-state-of-affairs\">Current State of Affairs</h2>\n<blockquote>\n<p><strong>Platform:</strong> Back to original motherboard → Intel i5-7400T, 32GB DDR4</p></blockquote>\n<blockquote>\n<p><strong>Case Solution:</strong> Rack shelf with underneath HDD mounting → 120mm fans in cabinet, test bench holder for motherboard</p></blockquote>\n<blockquote>\n<p><strong>Storage:</strong> HDDs screwed directly to shelf “mounting holes” → SSDs properly mounted this time</p></blockquote>\n<blockquote>\n<p><strong>Power:</strong> Back to proper UPS → Because no more burning through kilowatts while yelling at Home Assistant</p></blockquote>\n<blockquote>\n<p><strong>Container Management:</strong> Dockge on TrueNAS Scale → Health alerts gone after hardware revert</p></blockquote>\n<blockquote>\n<p><strong>LLM Inference:</strong> Migrated to OVH AI Endpoints → Home Assistant Assistant and Paperless-GPT</p></blockquote>\n<blockquote>\n<p><strong>Idle Power:</strong> Back to reasonable levels → 20W+ savings vs gaming PC setup</p></blockquote>\n<blockquote>\n<p><strong>Gaming PC:</strong> Listed on eBay → After 30+ years of always having one</p></blockquote>\n<h2 id=\"whats-next\">What’s Next</h2>\n<p>The setup is finally stable, properly rack-mounted, and not living behind a door with a bumper. The electricity bill is back to reasonable levels. The container health alerts are gone. And I’ve learned that sometimes the solution to “I need more compute” isn’t “throw gaming PC hardware at it.”</p><p>I’m still planning to write that Graylog article - the log aggregation rabbit hole was actually productive, even if the reason I went down it turned out to be hardware-related all along.</p><p>And who knows? Maybe I’ll miss having a dedicated gaming PC. But for now, GeForce Now and the M1 Mac Mini are doing just fine.</p><hr>\n<p><em>Have you ever “upgraded” your homelab only to end up right back where you started - but with three months of chaos in between? What’s the most ridiculous hardware decision you’ve made that seemed brilliant at the time?</em></p>",
            "image": "https://consolequest.net/media/posts/9/IMG_3953.jpeg",
            "author": {
                "name": "Juri Steiner"
            },
            "tags": [
                   "mistake",
                   "homelab",
                   "hardware"
            ],
            "date_published": "2026-01-20T23:18:50+01:00",
            "date_modified": "2026-01-21T14:40:40+01:00"
        },
        {
            "id": "https://consolequest.net/claude-code-docker.html",
            "url": "https://consolequest.net/claude-code-docker.html",
            "title": "Running Claude Code in Docker: Because SSH is All You Need",
            "summary": "Claude Code is excellent. It’s fast, it understands context, and it actually&hellip;",
            "content_html": "<p>Claude Code is excellent. It’s fast, it understands context, and it actually helps you write better code. It even supports remote authentication - you can grab an OAuth token via browser on any device and paste it in. There’s just one tiny problem: I’m weird and want to run it in a Docker container.</p><p>Containers don’t persist login state. Every restart, every rebuild - gone. Back to square one.</p><h2 id=\"why-run-claude-code-in-a-container\">Why Run Claude Code in a Container?</h2>\n<p>Fair question. Why not just run it locally like a normal person?</p><p>Because I work from way too many machines. Sometimes my desktop, sometimes my laptop - shit, maybe even from my work computer during breaks. I wanted one consistent environment I could SSH into from anywhere, with my notes and context already there.</p><p>Also, I’ll be honest: an AI with full access to my machine creeps me out a little. Inside a Docker container? Much better. It can do its thing in there, isolated, and I sleep easier.</p><p>And okay, fine - I also just wanted to see if it was possible.</p><h2 id=\"the-oauth-problem\">The OAuth Problem</h2>\n<p>When you start Claude Code, it asks: Claude subscription or API key?</p><p>Sounds like a choice. It’s not really. Both paths lead to OAuth - just to different endpoints:</p><ul>\n<li><strong>Claude subscription</strong> → OAuth via claude.ai</li>\n<li><strong>API key</strong> → OAuth via console.anthropic.com</li>\n</ul>\n<p>Either way: browser, login, authentication state that doesn’t survive a container restart.</p><p>But here’s the thing - you can set <code>ANTHROPIC_API_KEY</code> as an environment variable, and Claude Code will use it directly. No OAuth. This is actually documented for SDK usage. The catch? It still shows a confirmation dialog asking if you trust this key. In a container that rebuilds, you’d have to confirm every single time.</p><p>Unless you know where it stores that trust. That part isn’t in the docs.</p><h2 id=\"the-trust-config-discovery\">The Trust Config Discovery</h2>\n<p>After some digging (and reading way too much source code), I found that Claude Code stores trusted keys in <code>~/.claude.json</code>. The interesting part? It doesn’t store the full API key - just the last 20 characters for verification.</p><p>This means we can pre-populate the trust config:</p><pre><code class=\"language-json\">{\n  &quot;customApiKeyResponses&quot;: {\n    &quot;approved&quot;: [&quot;...last20chars...&quot;],\n    &quot;rejected&quot;: []\n  },\n  &quot;hasCompletedOnboarding&quot;: true,\n  &quot;hasTrustDialogAccepted&quot;: true\n}\n</code></pre>\n<p>Those flags skip the onboarding flow entirely. No browser needed, no interactive prompts, just straight to work.</p><h2 id=\"building-the-container\">Building the Container</h2>\n<p>The Dockerfile is straightforward - Ubuntu base, SSH server, Claude Code CLI:</p><pre><code class=\"language-dockerfile\">FROM ubuntu:24.04\n\n# System packages\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    openssh-server \\\n    ssh-import-id \\\n    curl git vim htop ripgrep jq \\\n    nodejs npm \\\n    &amp;&amp; rm -rf /var/lists/*\n\n# SSH setup - key-based auth only\nRUN mkdir /var/run/sshd \\\n    &amp;&amp; mkdir -p /root/.ssh \\\n    &amp;&amp; chmod 700 /root/.ssh \\\n    &amp;&amp; echo &quot;PermitRootLogin yes&quot; &gt;&gt; /etc/ssh/sshd_config \\\n    &amp;&amp; echo &quot;PasswordAuthentication no&quot; &gt;&gt; /etc/ssh/sshd_config \\\n    &amp;&amp; echo &quot;PubkeyAuthentication yes&quot; &gt;&gt; /etc/ssh/sshd_config\n\n# Claude Code CLI\nRUN npm install -g @anthropic-ai/claude-code\n\n# Workspace\nRUN mkdir -p /workspace/notes\n\nEXPOSE 22\nENTRYPOINT [&quot;/entrypoint.sh&quot;]\n</code></pre>\n<p>The entrypoint script is where the magic happens - it sets up SSH keys, configures the API key trust, and starts the SSH server:</p><pre><code class=\"language-bash\">#!/bin/bash\nset -e\n\n# Import SSH keys from GitHub\nif [ -n &quot;$GITHUB_USERS&quot; ]; then\n    ssh-import-id-gh ${GITHUB_USERS//,/ }\nfi\n\n# Claude Code API Key Setup\nif [ -n &quot;$ANTHROPIC_API_KEY&quot; ]; then\n    # Make key available in SSH sessions\n    echo &quot;export ANTHROPIC_API_KEY=\\&quot;$ANTHROPIC_API_KEY\\&quot;&quot; &gt;&gt; /root/.bashrc\n    \n    # Extract last 20 characters for trust config\n    ANTHROPIC_API_KEY_LAST_20=&quot;${ANTHROPIC_API_KEY: -20}&quot;\n    \n    # Create trust config\n    cat &lt;&lt;EOF &gt; /root/.claude.json\n{\n  &quot;customApiKeyResponses&quot;: {\n    &quot;approved&quot;: [&quot;$ANTHROPIC_API_KEY_LAST_20&quot;],\n    &quot;rejected&quot;: []\n  },\n  &quot;hasCompletedOnboarding&quot;: true,\n  &quot;hasTrustDialogAccepted&quot;: true\n}\nEOF\nfi\n\n# Start SSH server\nexec /usr/sbin/sshd -D\n</code></pre>\n<p>The <code>ssh-import-id-gh</code> command is criminally underrated - it pulls your public SSH keys directly from GitHub. No manual key management needed.</p><h2 id=\"then-i-wanted-to-push-to-github\">Then I Wanted to Push to GitHub</h2>\n<p>The container worked. Claude Code ran headless. I could SSH in and use it. Perfect.</p><p>Then I made some changes, felt productive, typed <code>git push</code> and… right. SSH keys. In a container. That I didn’t mount.</p><p>Sound familiar? Non-persistent authentication, round two. Apparently I needed to learn this lesson twice.</p><p>The problem is persistence:</p><pre><code class=\"language-yaml\">volumes:\n  - ./notes:/workspace/notes       # ✅ Persistent\n  - ./claude-data:/root/.claude    # ✅ Persistent\n  # But /root/.ssh?                # ❌ NOT mounted = gone on restart\n</code></pre>\n<p>SSH keys stored in <code>/root/.ssh</code> disappear on container restart. I could mount that directory too, but there’s a simpler solution: Git credential helper with a persistent location.</p><pre><code class=\"language-bash\">git config --global credential.helper &#39;store --file=/root/.claude/.git-credentials&#39;\n</code></pre>\n<p>Since <code>/root/.claude</code> is already bind-mounted, the credentials file survives container restarts. On first push, Git asks for credentials - I enter my GitHub Personal Access Token - and it’s stored. Every subsequent push works automatically.</p><p>This actually feels cleaner than SSH keys for containers. The token is revocable from GitHub’s settings, it doesn’t require key generation, and it works identically across any machine. Sometimes the workaround becomes the better solution.</p><h2 id=\"the-final-setup\">The Final Setup</h2>\n<p>Here’s the complete docker-compose configuration:</p><pre><code class=\"language-yaml\">services:\n  claude-code:\n    build: ./claude-ssh\n    container_name: claude-code\n    ports:\n      - &quot;2222:22&quot;\n    volumes:\n      - ./notes:/workspace/notes\n      - ./claude-data:/root/.claude\n    environment:\n      - GITHUB_USERS=${GITHUB_USERS}\n      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}\n    restart: unless-stopped\n</code></pre>\n<p>With a <code>.env</code> file:</p><pre><code class=\"language-bash\">GITHUB_USERS=your-github-username\nANTHROPIC_API_KEY=sk-ant-xxxxx\n</code></pre>\n<p>That’s it. Run <code>docker compose up -d --build</code>, wait about 5 minutes for npm to install Claude Code, then:</p><pre><code class=\"language-bash\">ssh -p 2222 root@your-server\ncd /workspace/notes\nclaude\n</code></pre>\n<p>No OAuth flow. No onboarding. No manual SSH key setup. Just Claude Code, ready to use, with working Git integration.</p><h2 id=\"bonus-pairing-with-a-markdown-editor\">Bonus: Pairing with a Markdown Editor</h2>\n<p>I run this alongside <a href=\"https://github.com/gamosoft/notediscovery\">NoteDiscovery</a>, a simple web UI for browsing Markdown files:</p><pre><code class=\"language-yaml\">services:\n  notediscovery:\n    image: ghcr.io/gamosoft/notediscovery:latest\n    ports:\n      - &quot;8800:8000&quot;\n    volumes:\n      - ./notes:/app/data\n\n  claude-code:\n    # ... as above\n</code></pre>\n<p>Both containers share the same notes directory. I browse and organize in the web UI, edit with Claude Code via SSH. It’s simple, it works, and I can access it from anywhere.</p><h2 id=\"current-setup\">Current Setup</h2>\n<blockquote>\n<p><strong>Container Platform:</strong> Docker Compose on TrueNAS Scale → Dockge for management</p></blockquote>\n<blockquote>\n<p><strong>Claude Code:</strong> Latest version via npm global install → API key auth, OAuth bypassed</p></blockquote>\n<blockquote>\n<p><strong>SSH Access:</strong> Port 2222 exposed → Keys automatically pulled from GitHub</p></blockquote>\n<blockquote>\n<p><strong>Persistence:</strong> Two bind mounts → notes/ and .claude/ directory, credentials survive restarts</p></blockquote>\n<blockquote>\n<p><strong>Git Integration:</strong> Personal Access Token → Stored in persistent location, works across rebuilds</p></blockquote>\n<blockquote>\n<p><strong>Companion Service:</strong> NoteDiscovery web UI → Same notes volume, different access method</p></blockquote>\n<h2 id=\"whats-next\">What’s Next</h2>\n<p>The setup works. But man, this thing churns through tokens like crazy. From jumping into the SSH session to pushing the <a href=\"https://github.com/neobiotics/claude-markdown-workspace\">GitHub repo</a> - okay, and making it barely presentable to the whole internet - five bucks were gone. </p><p>So I’m looking at <a href=\"https://github.com/anomalyco/opencode\">opencode</a> as an alternative. It has multi-provider support, so I could use cheaper models for simple tasks. Native web UI too, which might eliminate the SSH dance entirely. And maybe a Code Server container bolted on sideways at some point. We’ll see.</p><p>The whole setup is <a href=\"https://github.com/neobiotics/claude-markdown-workspace\">published on GitHub</a> if you want the complete config.</p><hr>\n<p><em>What’s your remote development setup look like? Are you running AI coding assistants in containers, or am I the only one who thought SSH-ing into a TUI was a good idea?</em></p>",
            "image": "https://consolequest.net/media/posts/8/Gemini_Generated_Image_fqvcf6fqvcf6fqvc-1.png",
            "author": {
                "name": "Juri Steiner"
            },
            "tags": [
                   "selfhosting",
                   "documentation",
                   "docker"
            ],
            "date_published": "2026-01-11T14:38:33+01:00",
            "date_modified": "2026-01-20T23:35:48+01:00"
        },
        {
            "id": "https://consolequest.net/the-backlog-of-doom-my-queue-of-ill-document-that-later-projects-2.html",
            "url": "https://consolequest.net/the-backlog-of-doom-my-queue-of-ill-document-that-later-projects-2.html",
            "title": "The Backlog of Doom: My Queue of &quot;I&#x27;ll Document That Later&quot; Projects",
            "summary": "You know what’s worse than having a messy homelab? Having a messy&hellip;",
            "content_html": "<p>You know what’s worse than having a messy homelab? Having a messy homelab that you haven’t documented yet. And you know what’s even worse than that? Publishing a blog post about all the things you haven’t documented, creating a public record of your procrastination.</p><p>So here we are.</p><h2 id=\"the-current-situation\">The Current Situation</h2>\n<p>I’ve got three TrueNAS servers, a UniFi network setup that’s probably overkill for a home environment, way too many Docker containers (still haven’t counted past 47), and a Home Assistant instance that controls more entities than I’d like to admit. And guess what? Almost none of it is properly documented.</p><p>Sure, I’ve got some scattered notes. A few Docker Compose files with comments like “did this because of that” Some mental notes about why I configured things a certain way. But actual documentation? The kind someone else could read and understand? Yeah, about that…</p><h2 id=\"the-i-need-to-document-this-yesterday-list\">The “I Need To Document This Yesterday” List</h2>\n<p>Let me break down what’s currently running undocumented in my infrastructure:</p><blockquote>\n<p><strong>Hardware Inventory:</strong> Three servers, various switches and APs → Honestly, it’s not going anywhere - I can just walk to the basement and look at it</p></blockquote>\n<p>The hardware is probably the easiest to document since it’s in a relatively finished state. It’s sitting there, doing its job, consuming electricity. At least if I forget what CPU is in which box, I can physically go look. But “finished” deserves documentation, right?</p><blockquote>\n<p><strong>Network Infrastructure:</strong> UniFi Gateway, Controller, switches, APs → The only thing truly “prod” in this setup - entire household depends on it</p></blockquote>\n<p>This is the one piece of infrastructure I’d actually call production-ready. It’s functional, optimized, and the whole household relies on it. When someone complains the internet is down, this is what I’m troubleshooting. It works well enough that I have planned changes and extensions, which is exactly why it’s appearing both in the “document what exists” and “projects to finish” categories.</p><blockquote>\n<p><strong>Smart Home:</strong> Home Assistant + various integrations → Works great, saves real money, but has rough edges I don’t discuss at parties</p></blockquote>\n<p>Here’s where it gets interesting. My Home Assistant setup barely makes this list because it <em>technically</em> works. The regular “lightswitch users” in my household don’t notice anything weird. In fact, they even get some comfort features they didn’t ask for. Most importantly, it’s saving actual money with automations around PV, power-to-heat, and heating.</p><p>But here’s the thing: it has so much more potential. There are rough edges. There are downright broken automations. Neglected integrations. Obsolete entities from experiments I forgot to clean up. If I’m being honest with myself - which apparently I am, in public, on the internet - this thing could use some serious attention.</p><h2 id=\"the-i-started-but-never-finished-projects\">The “I Started But Never Finished” Projects</h2>\n<p>Then there’s the category of projects I began with enthusiasm and then… didn’t finish:</p><p><strong>Network Segmentation &amp; Security Overhaul</strong><br>Even though my UniFi setup is production-stable, I have changes and extensions planned. Controller settings, Gateway configs, switch configurations, DNS setup, IDS/IPS rules. It’s all configured and working, but documenting it <em>before</em> making changes seems wise. Novel concept, I know. What am I planning to change? Proper network segmentation, a DMZ, VLANs with purpose, an IoT network with tailored radio settings, maybe even outbound firewall rules. You know, the stuff you read about and think “I should really do that properly.”</p><p><strong>Storage Architecture &amp; Backup Strategy Rework</strong><br>I’m currently in the midst of migrating folders for my Docker containers to separate datasets, optimized for the workload - databases on the SSD pool with appropriate record sizes, that sort of thing. The structure I’ve created is already too complicated, and I know it. Backups of irretrievable data exist, sure, but they follow no stringent logic and aren’t sensibly aligned with snapshot schedules. This all needs some Gehirnschmalz before it spirals further out of control.</p><p><strong>Smart Home Deep Dive</strong><br>This deserves its own post. Or maybe a series. Between the money-saving automations that work brilliantly and the abandoned experiments cluttering up my entity list, there’s a lot to unpack here.</p><h2 id=\"the-ambitious-plans-that-havent-started-category\">The “Ambitious Plans That Haven’t Started” Category</h2>\n<p>And then we have the projects that exist purely in the “this would be cool” phase:</p><p><strong>Centralized Logging (Graylog)</strong><br>Because apparently having logs scattered across multiple containers isn’t enterprise enough. I want to actually <em>aggregate</em> them and maybe even <em>search</em> them like a proper infrastructure.</p><p><strong>XDR/SIEM with Wazuh</strong><br>At some point, my homelab crossed the line from “hobby project” to “I should probably know if something suspicious is happening.” Wazuh keeps getting added to my “eventually” list.</p><p><strong>Remote Access Revamp (Netbird)</strong><br>Tailscale works, and I actually love it, but it always felt more at home at work. Netbird seems like it might fit better for a home setup. Plus, I want to give family members secure access to certain services without them needing a degree in networking.</p><p><strong>Long-term Data Retention (InfluxDB)</strong><br>I’m collecting metrics. Some of them are interesting. Some of them disappear after 30 days. InfluxDB would fix this, but it’s been on my list for months.</p><p><strong>Getting Actual Insights from My Data</strong><br>This is the ambitious one. Take all my data - logs, metrics, documents from Paperless-ngx, everything - and actually make it searchable and analyzable. RAG, knowledge graphs, maybe some ML. Yes, I know this sounds like I’m trying to build my own AI assistant. No, I haven’t started yet.</p><p><strong>N8N Workflow Automation</strong><br>Okay, maybe I <em>am</em> building my own AI assistant. N8N would tie everything together - automations that span services, intelligent workflows, data processing pipelines. It’s sitting on my “sounds amazing but haven’t installed it” list.</p><p><strong>Blog Analytics &amp; Comments</strong><br>Adding analytics and a comment section to this blog. Because scientifically speaking, zero readers and <em>exactly</em> zero readers are different things. Plus, this way I’m guaranteed to finally be the first person to comment on something for once.</p><p><strong>Home Energy Management System (HEMS)</strong><br>Solar, battery, heat pump, various loads, and §14a Module 3 compliance. For those not familiar with German energy regulations: this is about getting money back from the grid operator by giving them the ability to control certain devices during peak demand. It’s the project that keeps growing in scope every time I think about it. Integration of everything energy-related into one coherent system that actually optimizes for cost and grid stability.</p><h2 id=\"why-im-publishing-this\">Why I’m Publishing This</h2>\n<p>Two reasons:</p><p>First, public accountability. If I tell the internet (all zero of you reading this) that I’m going to document something, maybe I’ll actually do it.</p><p>Second, maybe someone else is in the same boat. Maybe you’ve also got a working setup that exists primarily in your memory and would take weeks to properly document. Maybe you’ve got automations saving you money while other automations from 2023 are still lingering in your config, disabled but not deleted. Maybe we can suffer through this together.</p><h2 id=\"the-plan-such-as-it-is\">The Plan (Such As It Is)</h2>\n<p>I’m going to tackle these in some semblance of order. Starting with documenting what’s already working, then finishing the half-done projects, then maybe - <em>maybe</em> - starting the ambitious new stuff.</p><p>But let’s be honest, I’ll probably get distracted by a new shiny project and add it to this list instead.</p><h2 id=\"current-status\">Current Status</h2>\n<blockquote>\n<p><strong>Blog Status:</strong> Two posts published → Infinite backlog created</p></blockquote>\n<blockquote>\n<p><strong>Documentation Tools:</strong> A collection of text files, a Flatnotes container, Apple Notes, and an unfinished Wiki.js install → Publii for publishing → Shit, I should learn Git to properly use GitHub for version control</p></blockquote>\n<blockquote>\n<p><strong>Current Focus:</strong> Avoiding work by writing about work → Classic procrastination technique</p></blockquote>\n<blockquote>\n<p><strong>Next Steps:</strong> Actually start checking things off this list → Electricity bill suggests urgency</p></blockquote>\n<h2 id=\"whats-next\">What’s Next</h2>\n<p>The hardware documentation post is probably first. Then the network setup, since that’s at least somewhat organized and actually production-stable. After that, we’ll see. Maybe I’ll finally count those containers. Or finally clean up those abandoned Home Assistant entities.</p><p>Or maybe I’ll add more to this list. One or the other.</p><hr>\n<p><em>What’s on your “I really should document this” list? And more importantly - what automations are you running that you’re slightly embarrassed about but refuse to delete because they might be useful someday?</em></p>",
            "image": "https://consolequest.net/media/posts/5/wonderlane-6jA6eVsRJ6Q-unsplash.jpg",
            "author": {
                "name": "Juri Steiner"
            },
            "tags": [
                   "projects",
                   "planning",
                   "homelab",
                   "documentation"
            ],
            "date_published": "2025-11-20T00:14:08+01:00",
            "date_modified": "2025-11-20T00:14:08+01:00"
        },
        {
            "id": "https://consolequest.net/console-quest-1-first-post-ever-no-really.html",
            "url": "https://consolequest.net/console-quest-1-first-post-ever-no-really.html",
            "title": "Console Quest #1: First Post Ever",
            "summary": "Hello World This is awkward. I’m starting a tech blog, but I’ve&hellip;",
            "content_html": "<h1 id=\"hello-world\">Hello World</h1>\n<p>This is awkward. I’m starting a tech blog, but I’ve never actually written a blog post before. Unless you count that LinkedIn rant about Burger King’s Pfand system where none of the staff knew they were supposed to take back reusable cups because apparently everyone just uses paper cups - which, let’s be honest, was peak content and deserved more engagement.</p><p>So here we are. My first real post. Ever.</p><h2 id=\"why-am-i-doing-this\">Why Am I Doing This?</h2>\n<p>Good question. I spend way too much time tinkering with selfhosted services, Docker containers, and automation setups. Currently running way too many containers across various services - I stopped counting after #47, but my electricity bill hasn’t. Honestly, most of this happens in isolation - me, a Dockge interface, and an increasingly complex TrueNAS setup that probably uses more electricity than it should.</p><p>I figured: why not document this chaos? Maybe someone else is on the same journey. Maybe not. Either way, at least I’ll have a record of all the times I broke my setup and somehow fixed it at 2 AM.</p><h2 id=\"the-console-quest-name\">The “Console Quest” Name</h2>\n<p>Yeah, it’s a bit dramatic. But it captures what this actually is - an ongoing quest. Not towards some perfect endpoint, but the journey itself. The tinkering, the next interesting project, the “I wonder if…” that leads to VLANs you didn’t plan to have.</p><p>It’s the quest that’s the point, not the destination.</p><p>Plus, “My Dozens of Containers Homelab Adventures” was too long for a domain name.</p><h2 id=\"why-english\">Why English?</h2>\n<p>But Juri you ask, you are german, why do you write in English, did you feel fancy today?\nActually, there is more than one reason for this.\nFirst and foremost, my brain thinks in english at the moment it comes to tech. 90% of what I read, watch or hear regarding tech is in english. Be it official documentation, Hardware Haven or Two and a Half Admins.\nSecond, I want to hone my english skills. Reading is good, but actually putting out something will hopefully do me some good in that regard.\nLast but not least, if I actually want to have a glimpse of hope at least someone is reading this, I better write it in some kind of “universal language”. And like it or not, that has to be english.</p><h2 id=\"but-youre-using-ai-right\">But you’re using AI, right?</h2>\n<p>Yeah, of course I am. I also use autocorrect. And a computer. I’m not gonna write my posts by hand, like a caveman. But while you’re asking: I write my posts myself, but heavily use AI to correct, structure and overall iterate over the posts. I also use it for research and for analysis.</p><h2 id=\"what-this-blog-will-be\">What This Blog Will Be</h2>\n<p>Honestly? Documentation of my ongoing homelab journey:</p><blockquote>\n<p><strong>Infrastructure Deep Dives:</strong> How I actually set things up → The choices, trade-offs, and why it works (or doesn’t)</p></blockquote>\n<blockquote>\n<p><strong>Home Automation Reality:</strong> Automations that save money and ones that… exist → Home Assistant’s messy truth</p></blockquote>\n<blockquote>\n<p><strong>Half-Finished Projects:</strong> Network segmentation plans, storage migrations, the things I started → Progress updates and lessons learned  </p></blockquote>\n<blockquote>\n<p><strong>“This Broke at 2 AM” Stories:</strong> Real troubleshooting → What went wrong and how I fixed it</p></blockquote>\n<blockquote>\n<p><strong>The Ambitious Ideas:</strong> Logging, monitoring, security → Projects that sound enterprise but it’s just my basement</p></blockquote>\n<p>No promises on posting schedule. Things will get documented when they’re ready - or when they break badly enough that I need to write it down.</p><h2 id=\"why-publii-and-not-ghost\">Why Publii (And Not Ghost)?</h2>\n<p>Speaking of services - this blog is powered by Publii, which is ironic since I literally have Ghost running in my homelab. But after spending my nights managing containers and configs, sometimes you just want a desktop app that generates static files and doesn’t require a database.</p><p>Don’t judge me! Doing it this way dramatically increases the likelihood of this staying up.</p><h2 id=\"current-setup\">Current Setup</h2>\n<p>Since we’re talking infrastructure, here’s what’s currently running this whole operation:</p><blockquote>\n<p><strong>Blog Platform:</strong> Publii static generator → Desktop app, no database drama</p></blockquote>\n<blockquote>\n<p><strong>Hosting:</strong> GitHub Pages → Free, reliable, plays nice with custom domains  </p></blockquote>\n<blockquote>\n<p><strong>Domain:</strong> consolequest.net → Because “MyContainersAreMultiplying.com” was taken</p></blockquote>\n<blockquote>\n<p><strong>Theme:</strong> Mono → Clean, fast, doesn’t get in the way of the content</p></blockquote>\n<blockquote>\n<p><strong>Homelab:</strong> TrueNAS Scale + Dockge → Where the real magic (and chaos) happens</p></blockquote>\n<h2 id=\"whats-next\">What’s Next</h2>\n<p>I’m planning to document my current stack - hardware, network, the services that work, the projects I started and abandoned. Maybe explain why certain choices made sense (like Dockge over Portainer). Probably write about the things that broke and how I fixed them.</p><p>We’ll see how long this lasts. My track record with “I’ll definitely document this properly” projects is a mixed bag.</p><p>But hey, at least I finally started.</p><hr>\n<p><em>Since I have literally zero readers right now, I’ll just say: if you somehow found this, hello! What’s running in your homelab? I’d love to hear about your own console quest adventures.</em></p>",
            "image": "https://consolequest.net/media/posts/2/matthieu-beaumont-iYnpYeyu57k-unsplash.jpg",
            "author": {
                "name": "Juri Steiner"
            },
            "tags": [
                   "truenas",
                   "selfhosting",
                   "introduction",
                   "homelab",
                   "docker"
            ],
            "date_published": "2025-05-24T23:43:13+02:00",
            "date_modified": "2025-11-22T20:43:20+01:00"
        }
    ]
}
